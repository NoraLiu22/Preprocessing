qefifsriicramqtipofsnevafofobcrccmrmeksg length 5 42581 page 42581 <!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/Article">
<head> 
<script type="text/javascript" src="/js/ga.js"></script>
<style type="text/css">

.topLeft	{	border-top: 1px solid #000000;
				border-left: 1px solid #000000;
				padding: 10px;
				 vertical-align: text-top;			
			}
			
.topLeftThick	{	border-top: 2px solid #000000;
				border-left: 1px solid #000000;
				 vertical-align: text-top;
			}
	        
.topLeftRight	{border-top: 1px solid #000000;
	         		  border-left: 1px solid #000000;
	         		  border-right: 1px solid #000000;
	         		  padding: 10px;
	         		  vertical-align: text-top;
			}

.topLeftRightThick	{border-top:  2px solid #000000;
	         		  border-left: 1px solid #000000;
	         		  border-right: 1px solid #000000;
	         		  vertical-align: text-top;
			}

.topLeftBottom	{border-top: 1px solid #000000;
	         			  border-left: 1px solid #000000;
	         			  border-bottom: 1px solid #000000;
	         			  padding: 10px;
	         			 vertical-align: text-top;
	        			}        
	     
.all	{border-top: 1px solid #000000;
	      border-left: 1px solid #000000;
	      border-bottom: 1px solid #000000;
	      border-right: 1px solid #000000;
	      padding: 10px;
	      vertical-align: text-top;
	     }	

table.plain {border-collapse: separate;
					border-spacing: 0px;
					margin-left: auto;
					margin-right: auto;					
					}
td.plain {padding: 6px;
			vertical-align: text-top;
					}

table.author {border-collapse: separate;
					border-spacing: 6px;
					}
td.authors {padding: 6px;
					}

li:not(:last-child) {
	margin-bottom: .5em;
	}
				
div.center {margin-left: auto; margin-right: auto;
	}

</style>
<meta charset="utf-8" />
<meta id="DOI" content="10.1045/january2017-wang" />
<meta itemprop="datePublished" content="2017-01-16" />
<meta id="description" content="D-Lib Magazine Article" /> 
<meta id="keywords" content="Provenance, Workflow, Big Data " />
<link href="../../../style/style1.css" rel="stylesheet" type="text/css" />

<title>Supporting Data Reproducibility at NCI Using the Provenance Capture System</title>
</head>

<body>
<form action="/cgi-bin/search.cgi" method="get">

<div style="height:2px;background:#2b538e"></div>
<div style="height:4px;background:#4078b1"></div>

<div style="height:30px;background:#4078b1">

<span style="color: #ffffff; font-size: 12px; float: right; margin-right: 10px;">Search D-Lib:
<input type="text" id="words" value="" size="25" />
<input type="submit" id="search" value="Go!" />
<input type="hidden" id="config" value="htdig" />
<input type="hidden" id="restrict" value="" />
<input type="hidden" id="exclude" value="" />
</span>
</div>

<div style="height:1px;background:#e04c1e"></div>
<div style="height:1px;background:#4078b1"></div>
<div style="height:1px;background:#abc0d6"></div>
<div style="height:2px;background:#4078b1"></div>
<div style="height:1px;background:#abc0d6"></div>
<div style="height:1px;background:#2b538e"></div>
<div style="height:92px;background:#4078b1"><img width="450" height="90" alt="D-Lib-blocks5" src="../../../img2/D-Lib-blocks5.gif">
</div>
<div style="height:1px;background:#abc0d6"></div>
<div style="height:2px;background:#4078b1"></div>
<div style="height:1px;background:#abc0d6"></div>
<div style="height:2px;background:#e04c1e"></div>
<div style="height:24px;background:#eda443"><img src="../../../img2/magazine5.gif" alt="The Magazine of Digital Library Research" width="830" height="24" /></div>
<div style="height:1px;background:#e04c1e"></div>
<div style="height:28px;background:#2b538e">
<div id="navtable">
<table>
<tr><td class="navtext"><img src="../../../img2/transparent.gif" alt="" width="20" height="20" /><a href="../../../dlib.html">HOME</a>&nbsp;|&nbsp;<a href="../../../about.html">ABOUT D-LIB</a>&nbsp;|&nbsp;<a href="../../../contents.html" class="navtext">CURRENT ISSUE</a>&nbsp;|&nbsp;<a href="../../../back.html">ARCHIVE</a>&nbsp;|&nbsp;<a href="../../../author-index.html">INDEXES</a>&nbsp;|&nbsp;<a href="http://www.dlib.org/groups.html">CALENDAR</a>&nbsp;|&nbsp;<a href="../../author-guidelines.html">AUTHOR GUIDELINES</a>&nbsp;|&nbsp;<a href="http://www.dlib.org/mailman/listinfo/dlib-subscribers">SUBSCRIBE</a>&nbsp;|&nbsp;<a href="../../letters.html">CONTACT D-LIB</a></td></tr></table></div></div>
<div style="height:4px;background:#2b538e"></div>
<div style="height:1px;background:#e04c1e"></div>

<div style="padding-left: 2.5em; padding-top: 1em;">

<h3 class="blue-space">D-Lib Magazine</h3>
<p class="blue">January/February 2017<br />
Volume 23, Number 1/2<br />
<a href="../01contents.html">Table of Contents</a>
</p> 

<div class="divider-full">&nbsp;</div>

<h3 class="blue-space">Supporting Data Reproducibility at NCI Using the Provenance Capture System</h3>

<p class="blue">
Jingbo Wang<br />
National Computational Infrastructure, Australia<br />
<a href="http://orcid.org/0000-0002-3594-1893"  class="nolinka">http://orcid.org/0000-0002-3594-1893</a><br />
jingbo.wang&#064;anu.edu.au<br /><br />

Nick Car<br />
Geoscience Australia<br />
<a href="http://orcid.org/0000-0002-8742-7730"  class="nolinka">http://orcid.org/0000-0002-8742-7730</a><br />
nicholas.car&#064;ga.gov.au<br /><br />

Edward King<br />
CSIRO, Australia<br />
<a href="http://orcid.org/0000-0002-6898-2310"  class="nolinka">http://orcid.org/0000-0002-6898-2310</a><br />
edward.king&#064;csiro.au<br /><br />

Ben Evans<br />
National Computational Infrastructure, Australia<br />
<a href="http://orcid.org/0000-0002-6719-2671"  class="nolinka">http://orcid.org/0000-0002-6719-2671</a><br />
ben.evans&#064;anu.edu.au<br /><br />

Lesley Wyborn<br />
National Computational Infrastructure, Australia<br />
<a href="http://orcid.org/0000-0001-5976-4943"  class="nolinka">http://orcid.org/0000-0001-5976-4943</a><br />
lesley.wyborn&#064;anu.edu.au 
</p>

<p class="blue">Corresponding Author: Jingbo Wang, jingbo.wang&#064;anu.edu.au</p> 

<div class="divider-dot">&nbsp;</div>

<p><a href="https://doi.org/10.1045/january2017-wang" class="nolinka">https://doi.org/10.1045/january2017-wang</a></p>

<div class="divider-full">&nbsp;</div>
 <!-- Abstract or TOC goes here --> 

<h3 class="blue">Abstract</h3>

<p class="blue">Scientific research is published in journals so that the research community is able to share knowledge and results, verify hypotheses, contribute evidence-based opinions and promote discussion. However, it is hard to fully understand, let alone reproduce, the results if the complex data manipulation that was undertaken to obtain the results are not clearly explained and/or the final data used is not available. Furthermore, the scale of research data assets has now exponentially increased to the point that even when available, it can be difficult to store and use these data assets. In this paper, we describe the solution we have implemented at the National Computational Infrastructure (NCI) whereby researchers can capture workflows, using a standards-based provenance representation. This provenance information, combined with access to the original dataset and other related information systems, allow datasets to be regenerated as needed which simultaneously addresses both result reproducibility and storage issues.</p>

<p class="blue">Keywords: Provenance, Workflow, Big Data</p>

<!-- Article goes next --> 

<div class="divider-full">&nbsp;</div>
<h3>1 Introduction</h3>

<p>The National Computational Infrastructure (NCI) at the Australian National University (ANU) has co-located over 10 PBytes of national and international Earth Systems and Environmental data assets within a High Performance Computing (HPC) facility, which is accessible through the National Environmental Research Data Interoperability Platform (NERDIP) [<a href="#1" >1</a>]. Datasets are either produced at the NCI, stored from instruments, or replicated as needed from external locations. In many cases this data is processed to higher-level data products which are also stored and published through NERDIP. The datasets within these collections can range from multi-petabyte climate models and large-volume raster arrays, down to gigabyte size, ultra-high resolution datasets. All data collections are currently in the process of being quality assured in order to be made available via the web services. Most data collections at the NCI are dynamic &#151; datasets grow spatially or temporally, models/derivative products are revised and rerun, and whole datasets are reprocessed as analysis methods are improved. Additionally, some products require the generation of multiple intermediate data sets. At the scale of NCI it is not easy to justify storing all the previous versions of datasets and intermediate datasets or to of track, store and republish all the subsets of datasets generated and referred in publications. </p>

<p>To address these issues, we decided to integrate a provenance information service. Provenance information can be defined as "information about entities, activities, and people involved in producing a piece of data or thing" [<a href="#2">2</a>,<a href="#3">3</a>]. Through the implementation that is described below, we have enabled a service that allows us to fully describe how datasets or subsets have been generated, and that allows the history of such datasets to be referenced, such as for methodological citation within third-party publications. This is particularly useful for authors submitting papers to high-impact journals such as <i>Nature</i> and <i>Science</i> where space is at a premium [<a href="#4">4</a>]. A common example of provenance is scientific workflow. "Scientific workflows provide the means for automating computational scientific experiments, enabled by established workflow execution engines, like <a href="https://taverna.incubator.apache.org/">Apache Taverna</a>, <a href="https://kepler-project.org/">Kepler</a>, <a href="http://www.vistrails.org/">VisTrails</a> or <a href="https://galaxyproject.org/">Galaxy</a>. A <a href="http://dx.doi.org/10.1016/j.websem.2015.01.003">workflow-centric research object</a> contain the application-specific workflow definition, annotated with <a href="http://purl.org/wf4ever/wfdesc">wfdesc</a>, and combined with a <a href="http://www.w3.org/TR/prov-o/">PROV</a>-based execution trace of one or more <i>workflow runs</i>, including <i>inputs</i> and <i>outputs</i>" (see <a href="http://www.researchobject.org">researchobject.org</a>) [<a href="#5">5</a>]. </p>

<p>The NCI represents provenance information from all processes according to a standardised data model known as PROV-DM [<a href="#2">2</a>]. The information is stored as <a href="https://www.w3.org/RDF/">RDF</a>  documents in an RDF graph database, which is then published via a <a href="https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol">HTTP-based</a> API as a combination of web pages and data. We describe how a provenance information service is implemented at NCI. Then we demonstrate a provenance use case using satellite images processing applied in oceanography to show the workflow and provide proof that the whole process can be repeated by following the detailed provenance report. Providing provenance information also supports the quality assurance and quality control (QA/GC) processes for the NCI, which ensures that there is a trust-worthy, standardised and fully-described record that can be integrated within programmatic workflows that can then be reported and published via a centralised information service. </p>

<div class="divider-full">&nbsp;</div>
<h3>2 Methodology</h3>

<p>We list the generic steps required for establishing provenance capture at NCI:</p>

<ul>
	<li><b>Systems</b> &#151; establishment of central provenance management infrastructure;</li>

	<li><b>Modelling</b>  &#151; of workflows/processing for which we require provenance. Modelling must be in accordance with PROV-O [<a href="#6">6</a>] standard for interoperability;</li>

	<li><b>Data management</b> &#151; of the data which the workflow or process uses or generates. The data must be identified with URIs (see next section) stored as per the NCI's data management policy;</li>

	<li><b>Reporting implementation</b> &#151; the reporting process must record the modelled versions of the workflow; and</li>

	<li><b>Reporting operations</b> &#151; reporting from the process must continue each time the process is run. This can be accomplished when runs occur or after the event if the reporting implementation reads from log files or similar.</li>
</ul>

<p>In this paper, we focus on the <b>Systems</b>, <b>Modelling</b> and <b>Data management</b> component (see Section 3). Once established, this service can be used by multiple workflow and reporting processes. The others occur per-reporting process. To demonstrate the efficacy of this approach, we have provided a case study in Section 4 used for generation of ocean colour products using a computational pipeline on NCI's supercomputer.</p>

<div class="divider-full">&nbsp;</div>
<h3>3 NCI's Provenance Capture System</h3>

<div class="divider-dot">&nbsp;</div>
<h4>3.1 Provenance data model and architecture</h4>

<div style="text-align: center;">
<img style="margin: 10px 0px;" src="wang-fig1.png" alt="wang-fig1" width="478" height="269" />
<p><i>Figure 1: The basic classes and relationships of PROV-DM, drawn in accordance with the PROV Ontology [<a href="#3">3</a>].</i></p>
</div>

<p>The basic classes and relationships of PROV-DM are shown in Figure 1. Entities and Agents (things and people/systems) represented in the provenance information always have metadata representations both within and outside the provenance information management system. As such, an ecosystem of services is required. NCI uses a <a href="https://proms-dev.nci.org.au">PROMS Server</a> for its provenance information management system, a <a href="https://pid.nci.org.au">Persistent Identifier Service</a>, the NCI's <a href="https://datamgt.nci.org.au">Metadata Management System</a>  and an off-the-shelf <a href="https://cms.nci.org.au">content management system</a>. These are described in the following sections (3.2 and 3.3).</p>

<div class="divider-dot">&nbsp;</div>
<h4>3.2 PROMS Reporting System</h4>

<p>The core structure of the NCI provenance capture system is the Provenance Management System (PROMS) [<a href="#6">6</a>, <a href="#7">7</a>] which consists of an API and an underlying RDF store. The API is constrained to only accept <i>Reports</i> from registered <i>Reporting Systems</i> and ensure they are conformant both with PROV-O [<a href="#8">8</a>] and as well as NCI-specific metadata requirements.</p>

<p>Figure 2 shows a template information model for data processing at the NCI with common examples items found there. Process inputs could be datasets, algorithms, toolkits, software packages, or configuration files. The process is run by a user, or an instrument (such as an X-ray detector), or an automated workflow engine. The output of the process could be an artefact &#151; a report, datasets, images, research articles, or a video. Important metadata about the process being conducted (e.g., timestamp of the operation) are also captured in the provenance store. Information within the provenance store can be potentially tracked in real time so as to capture actions of a user's behavior, such as when a user runs executable code that performs data processing.</p>

<div style="text-align: center;">
<img style="margin: 10px 0px;" src="wang-fig2.png" alt="wang-fig2" width="560" height="333" />
<p style="text-align: center; margin: 0px 80px;"><i>Figure 2: A template information model for data processing, conformant with PROV-O.<br />The class images are those from Figure 1 and their labels indicate real examples of those class instances found at the NCI.</i></p>
</div>

<p>Figure 3 demonstrates linear and nonlinear data process scenarios. PROMS can capture both very well. </p>

<div style="text-align: center;">
<img style="margin: 10px 0px;" src="wang-fig3.png" alt="wang-fig3" width="727" height="183" />
<p><i>Figure 3: Flowchart of different types of workflows captured by PROMS. </i></p>
</div>

<p>The PROMS Server supports three levels of provenance information reporting, all of which are represented as PROV-O-compliant documents: </p>
<ol>
	<li><i>Basic</i>: contains a title, basic process metadata such as a starting and ending time and is mainly used for testing.</li>
	<li><i>External</i>: a high-level process description, including a listing of the data consumed and produced by a process: the detailed process is described as a black box. </li>
	<li><i>Internal</i>: provides details of the internal steps and intermediate datasets of a process, as well as the data consumed and produced.</li>
</ol>
<p>Figure 4 shows a graphical representation of the information in a PROMS <i>External</i> report, as shown in a web page by the PROMS Server. The Entity, Activity and Agent objects are linked to queries so that, when clicked in a web browser, the diagram is dynamically redrawn with the clicked object as the focus. Links for objects with metadata in dataset or agent catalogues lead directly to those representations. The NCI manages persistent link (<a href="https://en.wikipedia.org/wiki/Uniform_Resource_Identifier">URI</a>) services, and a series of  metadata repositories and content management systems that are the targets of dataset and agent links.</p>

<div style="text-align: center;">
<img style="margin: 10px 0px;" src="wang-fig4.png" alt="wang-fig4" width="560" height="384" />
<p><i>Figure 4: Sample External report displayed on PROMS interface</i></p>
</div>

<div class="divider-dot">&nbsp;</div>
<h4>3.3 Data Management Services</h4>

<p>A well-managed, mature data management system is essential for provenance information to be valuable. It becomes almost impossible to confirm that a product was produced with the correct data if that data is not well-managed [<a href="#9">9</a>]. NCI uses  a number of tools and services as part of its data management infrastructure that are used by the provenance system:</p>

<p><i>Metadata catalogue</i>. NCI has implemented metadata catalogues in a hierarchical structure so that they are both extensible and scalable. To support collection-level data management, a Data Management Plan (DMP) has been developed to record workflows, procedures, key contacts and responsibilities. The elements of the DMP are recorded in an ISO19115 [<a href="#10">10</a>] compliant record that is made available through a catalogue for metadata display and exchange [<a href="#11">11</a>]. Figure 5 shows the NCI's catalogue hierarchy. The <a href="https://geonetwork.nci.org.au">top-level catalogue</a> hosts collection/dataset level metadata, which can be harvested by other data aggregators. Each project then has a specific instance hosting more granular metadata. The lower-level catalogues are given host names according to the pattern of geoNetwork{NCI-PROJECT-CODE}.nci.org.au (e.g., https://geonetworkrr9.nci.org.au/).</p>

<div style="text-align: center;">
<img style="margin: 10px 0px;" src="wang-fig5.png" alt="wang-fig5" width="480" height="423" />
<p><i>Figure 5: Scalable GeoNetwork infrastructure to support various metadata display and exchange purposes.</i></p>
</div>

<p><i>Internal Content Repository</i>. Within an <i>Internal</i> PROMS report, many internal datasets, documents, and images will be represented. Most of the time they are not intended for external publication, therefore they are not stored in a product catalogue, nor do they have an NCI product identifier (such as <a href="https://www.doi.org">DOI</a>) minted for them. However, in order to preserve complete provenance information, some of these internal items such as documents, are stored in a Content Management System (<a href="https://cms.nci.org.au">CMS</a>) and NCI URIs are minted to identify them. </p>

<p><i>Persistent Identifier Services</i>. The NCI has a Persistent Identifier Service (PID Service) [<a href="#12">12</a>] to manage the Uniform Resource Identifier (URI) of the entities. Persistent identifier is an integral part of the semantic web and linked data applications. The  service enables a stable reference for persistent identifiers using an approach that intercepts all incoming HTTP requests at the Apache HTTP web server level, passes them through to the PID Service dispatcher servlet which then implements a logic to recognize a pattern of an incoming request, compares it with one of the patterns configured in the PID Service that is stored in a persistent relational data store, and performs a set of user defined actions. The service allows URI tracking through a dedicated interface, database lookups and rule mapping lists. NCI's PID service is configured [<a href="#13">13</a>] as a broker to map between original URL and uniformed URI. It applies to catalogue entries, data services endpoints, content management system entries, as well as vocabulary service (see Figure 6). </p>

<div style="text-align: center;">
<img style="margin: 10px 0px;" src="wang-fig6.png" alt="wang-fig6" width="680" height="315" />
<p><i>Figure 6: PIDs service infrastructure.</i></p>
</div>

<p>Dataset URIs follow the pattern http://pid.nci.org.au/dataset/{ID}. This URL will map to the individual dataset's catalogue URLs, e.g., https://geonetwork.nci.org.au/geonetwork/srv/eng/catalog.search#/metadata/{ID}. The catalogue {ID}s used are universally unique identifiers (UUID), which means the catalogue entries they identify can be easily moved between different catalogue instances so that the corresponding persistent URI remain the same, with only the redirection mapping needing to be changed. The multiple catalogues' entries are harvested into the PID Service's lookup tables, so the dataset URI mappings can be automatically made. The PID Service uses a dedicated database server to perform mapping lookups. This is a much faster technique for large numbers of mappings than the alternative through Apache or application code [<a href="#12">12</a>]. Figure 7 shows the example UUID and PID URI mapping which takes precedence over the URI pattern based mapping. A script has been written to harvest the multiple catalogues' entries into the PID service thus keeping URI redirects always up-to-date.</p>

<div style="text-align: center;">
<img style="margin: 10px 0px;" src="wang-fig7.png" alt="wang-fig7" width="732" height="561" class="border" />
<p><i>Figure 7: Screenshot of the lookup table when multiple GeoNetwork instances exist. </i></p>
</div>
<div style="height:12px;background:#ffffff"></div>

<div class="divider-full">&nbsp;</div>
<h3>4 Ocean Colour Data Processing Application</h3>

<p>We now provide an example of how a scientific workflow has been recorded using NCI's provenance capture system. </p>

<div class="divider-dot">&nbsp;</div>
<h4>4.1 Overview</h4>

<p>Ocean colour observations are multispectral measurements, made from Earth observing satellites, of sunlight reflected from within the surface layer of the ocean. Under particular circumstances the spectral observations can be used to quantify the concentrations and properties of optically active substances in the water. The predominant source of such measurements over the past 14 years has been the Moderate Resolution Imaging Spectroradiometer (MODIS) sensor carried on the NASA/Aqua spacecraft. As part of the <a href="https://research.csiro.au/ereefs/remote-sensing/data-processing/">eReefs Marine Water Quality monitoring program</a>, the Commonwealth Scientific and Industrial Research Organisation (CSIRO) developed ocean colour algorithms adapted specifically to the environment of the Great Barrier Reef (GBR) region to derive in-water concentrations of chlorophyll-a, sediments and coloured dissolved organic matter from MODIS/Aqua observations. These algorithms, which first compensate for the effects of the atmosphere and then retrieve the in-water concentrations, rely critically upon correctly calibrated satellite observations as input, and use the Australian MODIS time series collection supported by IMOS at the NCI. </p>

<div class="divider-dot">&nbsp;</div>
<h4>4.2 Ocean Colour Workflow</h4>

<p>The complete data processing workflow consists of several steps, including preparation of the input data, application of the customised algorithms, and post-processing to facilitate their use by other researchers. These individual steps are enumerated below and shown in Figure 8 (see [<a href="#14">14</a>] for a complete description):</p>

<ol>
	<li>Assembling the data (Level-0 granules);</li>

	<li>Calibration and geolocation (Level-0 to Level-1b);</li>

	<li>Generation of ancillary fields (Level-1b to Level-2);</li>

	<li>Data selection and masking;</li>

	<li>Atmospheric correction;</li>

	<li>In-water inversion (deriving concentrations of optically active substances);</li>

	<li>Regridding (conversion of satellite grids to map projection);</li>

	<li>Mosaicing (joining multiple satellite images to provide more complete coverage);</li>

	<li>Data distribution and access.</li>
</ol>

<p>Steps 1-3 are part of a separate data processing workflow that precedes the GBR-specific analyses and are not included in the provenance reporting discussion here. For practical processing operational reasons, the provenance modelling groups the remaining steps 4-10 into three stages (see <a href="#appendix">Appendix</a> Figure 1-3). </p>

<div style="text-align: center;">
<img style="margin: 10px 0px;" src="wang-fig8.png" alt="wang-fig8" width="560" height="657" />
<p><i>Figure 8: Remote sensing <a href="http://research.csiro.au/ereefs/wp-content/uploads/sites/34/2015/08/workflowDiagram.png">data processing workflow</a>.</i></p>
</div>
<div style="height:12px;background:#ffffff"></div>

<div class="divider-dot">&nbsp;</div>
<h4>4.3 Provenance Report</h4>
 
<p>The goal of the provenance reports in this case is to help the user rerun the ocean colour processing workflow. Figure 9 is the screenshot of an <i>External</i> view of a report as shown by the PROMS Server. Users are able to click each entity to drill down and find what datasets/code are available for access.</p>

<div style="text-align: center;">
<img style="margin: 10px 0px;" src="wang-fig9.png" alt="wang-fig9" width="560" height="432" />
<p><i>Figure 9: Ocean Colour data process provenance report.</i></p>
</div>
 
<p>One of the key components in the provenance report is the input entity URIs which uniquely identify the versions (and locations) of processing codes, datasets, and ancillary data so that all the initial information required to re-run the processing can be obtained. For example, the ocean colour processing uses the data that is available through NCI's NERDIP OPeNDAP data service through endpoint URLs that are the input dataset entity's value. The agent is the researcher who ran the process. We use the researcher's <a href="http://orcid.org">ORCID</a>  identifier, which provides information about the researcher, such as their publications, institution, role, and other relevant research characteristics. The outputs of the processing are generated and stored at NCI. In this application, the Ocean Colour product as well as the product description are the output entities of the provenance report. The Ocean Colour product dataset is published through the NCI OPeNDAP service. The dataset also has a catalogue entry stored in NCI's metadata repository. In that case, the URI minted using NCI's PID service (see Section 3.3) is used as the dataset catalogue URI. When a user clicks that URI, the user is redirected to NCI's metadata GeoNetwork catalogue. The URI for the ocean color processing code is the URL of CSIRO source code repository. </p>

<p>A list of different types of provenance reports is available from <a href="https://proms.nci.org.au">NCI's provenance server</a>. Users can select one that matches the level of process granularity that they are interested in.</p>

<div class="divider-dot">&nbsp;</div>
<h4>4.4 Staff Resource Effort</h4>

<p>The staff resources needed to establish the Ocean Colour reporting indicated in Section 2 were: </p>

<ul>
	<li>Part-time work over several months to implement the central information services;</li>

	<li>Several days work from three people (an Ocean Colour product owner and modellers) to model the Ocean Colour process as per PROV-O;

	<li>Reorganisation of the Ocean Colour workflow itself for better modularisation within the lifetime of this project which eased modelling;</li>

	<li>A week to implement logging within the Ocean Colour workflow that could be used to establish PROV-O reports from each workflow run;</li>

	<li>A small amount of work for data management, as the Ocean Colour workflow was already using managed NCI storage for inputs and outputs &#151; storage location URIs were able to be used directly; and</li>

	<li>A day or two to schedule reporting to work continuously.</li>
</ul>

<div class="divider-full">&nbsp;</div>
<h3>5 Summary and Discussion</h3>

<p>Reviewers and users of any research publication that is based on data processing or digital laboratory experiments will need to be able to independently determine how any published results have been calculated. The key is to share the end-to-end workflow with the audience so that the paper is transparent and results are reproducible. NCI has built a comprehensive provenance infrastructure to support transparency of research and to varying levels, reproducibility. The provenance report enables the transparency required by following the PROMS report. </p>

<p>We have demonstrated that our approach is a significant enhancement of that provided in a traditional research paper.  We have demonstrated within a real-world example using the Ocean Colour data processing workflow. The overall provenance capture system and its supporting repository management, such as data catalogs and an internal content management system, provides the necessary information ecosystem to support this use-case. We also confirmed how persistent identifier services have increased the inherent robustness of the system by having UUID as the key element, instead of using vulnerable URLs. With all the codes, source data, and configuration information within the system, users are able to see how each dataset was produced. The internal provenance report clearly defines every step of the process.</p>

<p>The next workflow to be implemented at the NCI is a genomics data processing pipeline. While this workflow is completely different to that used for the Ocean Colour process, it will also use the catalogues, data and provenance storage systems described here. Mapping of the genomics workflow to PROV-O has commenced and is proceeding smoothly. This indicates that other systems that require provenance reporting need only concentrate on their system-specific parts of the method outlined in Section 2.</p>

<div class="divider-full">&nbsp;</div>
<h3>References</h3>

<table style="width:90%">
<tr>
<td style="padding-bottom: 12px; vertical-align: top;"><a id="1">[1]</a></td>
<td style="padding-bottom: 12px; vertical-align: top;">Evans, B., Wyborn, L., Pugh, T., Allen, C., Antony, J., Gohar, K., Porter, D., Smillie, J., Trenham, C., Wang, J., Ip, A., and Bell, G., 2015. The NCI High Performance Computing and High Performance Data Platform to Support the Analysis of Petascale Environmental Data Collections. <i>Environmental Software Systems Infrastructures</i>, Services and Applications, (pp. 824-830). 11th IFIP WG 5.11 International Symposium, ISESS 2015 Melbourne, VIC, Australia, March 25–27, 2015.</td>
</tr>
<tr>
<td style="padding-bottom: 12px; vertical-align: top;"><a id="2">[2]</a></td>
<td style="padding-bottom: 12px; vertical-align: top;">Moreau, L., and Missier, P., 2013. <a href="https://www.w3.org/TR/prov-dm/">PROV-DM: The PROV data model</a>. World Wide Web Consortium Recommendation, 30 April 2013.</td>
</tr>
<tr>
<td style="padding-bottom: 12px; vertical-align: top;"><a id="3">[3]</a></td>
<td style="padding-bottom: 12px; vertical-align: top;">Hartig, O., 2009. <a href="http://events.linkeddata.org/ldow2009/papers/ldow2009_paper18.pdf">Provenance Information in the Web of Data</a>. LDOW, 538.</td>
</tr>
<tr>
<td style="padding-bottom: 12px; vertical-align: top;"><a id="4">[4]</a></td>
<td style="padding-bottom: 12px; vertical-align: top;">Baker, M., 2016. <a href="http://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970">1,500 scientists lift the lid on reproducibility</a>. <i>Nature</i>, 533, 452-454.</td>
</tr>

<tr>
<td style="padding-bottom: 12px; vertical-align: top;"><a id="5">[5]</a></td>
<td style="padding-bottom: 12px; vertical-align: top;">Belhajjame, K., Zhao, J., Garijo, D., Gamble, M., Hettne, K., Palma, R., Mina, E., Corcho, O., G&#243;mez-P&#233;rez, J.M., Bechhofer, S., Klyne, G., Goble, C., 2015. Using a suite of ontologies for preserving workflow-centric research objects, <i>Web Semantics</i>: Science, Services and Agents on the World Wide Web. <a href="https://doi.org/10.1016/j.websem.2015.01.003">https://doi.org/10.1016/j.websem.2015.01.003</a>

</td>
</tr>

<tr>
<td style="padding-bottom: 12px; vertical-align: top;"><a id="6">[6]</a></td>
<td style="padding-bottom: 12px; vertical-align: top;">Car, N., 2013. A method and example system for managing provenance information in a heterogeneous process environment &#151; a provenance architecture containing the Provenance Management System (PROMS). <i>20th International Congress on Modelling and Simulation, Adelaide, Australia</i>. Adelaide: 20th International Congress on Modelling and Simulation.</td>
</tr>

<tr>
<td style="padding-bottom: 12px; vertical-align: top;"><a id="7">[7]</a></td>
<td style="padding-bottom: 12px; vertical-align: top;">Car, N., 2014. "<a href="http://eresearchau.files.wordpress.com/2014/07/eresau2014_submission_60.pdf">Inter-agency standardised provenance reporting in Australia</a>". eResearch Australasia Conference, October 27 - 30, 2014, Melbourne Australia.</td>
</tr>

<tr>
<td style="padding-bottom: 12px; vertical-align: top;"><a id="8">[8]</a></td>
<td style="padding-bottom: 12px; vertical-align: top;">Lebo, T., Sahoo, S.,  and McGuinness, D., 2013.  "<a href="https://www.w3.org/TR/prov-o/">PROV-O: The PROV Ontology</a>," W3C recommendation, 2013.</td>
</tr>

<tr>
<td style="padding-bottom: 12px; vertical-align: top;"><a id="9">[9]</a></td>
<td style="padding-bottom: 12px; vertical-align: top;">Fitch P., Car, N., and Lemon, D., 2015. <a href="https://publications.csiro.au/rpr/pub?pid=csiro:EP153321">Organisational provenance capacity implementation plan: a report for Geoscience Australia</a>. CSIRO.</td>
</tr>

<tr>
<td style="padding-bottom: 12px; vertical-align: top;"><a id="10">[10]</a></td>
<td style="padding-bottom: 12px; vertical-align: top;">ISO (2015). "<a href="http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=53798">ISO19115-1:2014. Geographic information &#151; Metadata &#151; Part 1: Fundamentals</a>". Standards document. (Online paywalled). International Organization for Standardization, Geneva.</td>
</tr>

<tr>
<td style="padding-bottom: 12px; vertical-align: top;"><a id="11">[11]</a></td>
<td style="padding-bottom: 12px; vertical-align: top;">Wang, J., Evans, B., Bastrakova, I., Ryder, G.,  Martin, J., Duursma, D., Gohar, K., Mackey, T., Paget, M., Siddeswara, G., and Wyborn, L., 2014. Large-Scale Data Collection Metadata Management at the National Computation Infrastructure. American Geophysical Union Fall meeting, San Francisco, USA, December 13-17, 2014.</td>
</tr>

<tr>
<td style="padding-bottom: 12px; vertical-align: top;"><a id="12">[12]</a></td>
<td style="padding-bottom: 12px; vertical-align: top;">Golodoniuc, P., 2013. <a href="https://www.seegrid.csiro.au/wiki/Siss/PIDService">Persistent Identifier Service (PID Service)</a>. Wiki web page by the Solid Earth and Environment GRID.</td>
</tr>

<tr>
<td style="padding-bottom: 12px; vertical-align: top;"><a id="13">[13]</a></td>
<td style="padding-bottom: 12px; vertical-align: top;">Wang, J., Car, N., Si, W., Evans, B., and Wyborn, L.,  2016. <a href="https://eresearchau.files.wordpress.com/2016/03/eresau2016_paper_90.pdf">Persistent Identifier Practice for Big Data Management at NCI</a>. eResearch Australasia 2016.</td>
</tr>

<tr>
<td style="padding-bottom: 12px; vertical-align: top;"><a id="14">[14]</a></td>
<td style="padding-bottom: 12px; vertical-align: top;">King, E., Schroeder, T., Brando, V., and Suber, K., 2013. "<a href="https://publications.csiro.au/rpr/pub?pid=csiro:EP144849">A Pre-operational System for Satellite Monitoring of Great Barrier Reef Marine Water Quality</a>", Wealth from Oceans Flagship report.</td>
</tr>
</table>

<div class="divider-full"><a id="appendix">&nbsp;</a></div>
<h3>Appendix</h3>

<p>The figures below show the provenance modelling grouped into three stages. </p>

<div style="text-align: center;">
<img style="margin: 10px 0px;" src="wang-appendix-fig1.png" alt="wang-appendix-fig1" width="708" height="823" />
<p><i>Figure 1: Select the data, apply the mask and run the ANN.</i></p>
</div>
<div style="height:12px;background:#ffffff"></div>
<div style="height:1px;background:#cccccc"></div>
<div style="height:12px;background:#ffffff"></div>

<div style="text-align: center;">
<img style="margin: 10px 0px;" src="wang-appendix-fig2.png" alt="wang-appendix-fig2" width="574" height="453" />
<p><i>Figure 2: Run the aLMI on the individual outputs of Figure 1.</i></p>
</div>
<div style="height:12px;background:#ffffff"></div>
<div style="height:1px;background:#cccccc"></div>
<div style="height:12px;background:#ffffff"></div>

<div style="text-align: center;">
<img style="margin: 10px 0px;" src="wang-appendix-fig3.png" alt="wang-appendix-fig3" width="528" height="879" />
<p><i>Figure 3: Take ANN outputs and remap and mosaic. There is also an NPP step which maybe should be separate.</i></p>
</div>
<div style="height:12px;background:#ffffff"></div>

<div class="divider-full">&nbsp;</div>
<h3>About the Authors</h3>

<p class="blue"><b>Jingbo Wang</b> is the Data Collections Manager at the National Computational Infrastructure where she is leading the migration of data collections onto the RDS (Research Data Service) funded filesystems. Dr. Wang's focus is on building the infrastructure to support data management, data citation, data ingest publishing logistics and provenance capture system. She is also interested in how to provide the best data services to the research community through provenance, graph database, etc., technology. As a geophysicist, she is also working and how to advance the science through interdisciplinary research that combines the HPC/HPD platform with the massive geophysical data collection at NCI.</p>

<div class="divider-full">&nbsp;</div>

<p class="blue"><b>Nicholas Car</b> is the Data Architect for Geoscience Australia, Australia's geospatial science government agency. He formerly worked as an experimental computer scientist at the CSIRO, building semantic web and other IT systems to manage government and research data. At GA his role is to provide advice to the agency on its data management and systems and to lead the data modelling team. His research interests include information modelling, provenance and the semantic web, all three of which he believes are vital for transparent and reproducible digital science. He is heavily involved with inter-agency and international metadata and Linked Data collaborations including co-chairing the Research Data Alliance's Research Data Provenance Interest Group and as a member of the Australian Government Linked Data Working Group. He is tasked with delivering an internal 'Enterprise Data Model' for GA and its corresponding external (public) representation.</p>

<div class="divider-full">&nbsp;</div>

<p class="blue"><b>Edward King</b> leads the ocean remote sensing team at CSIRO in Hobart, Tasmania. Dr. King is a specialist in the management and exploitation of large scale remote sensing time series in both terrestrial and marine applications, particularly using the NCI. As a task leader in the multi-institution eReefs partnership, he was responsible for applying regionally tuned optical remote sensing algorithms to a 14-year collection of daily satellite imagery to assess water quality on Australia's Great Barrier Reef.</p>

<div class="divider-full">&nbsp;</div>

<p class="blue"><b>Ben Evans</b> is the Associate Director of Research, Engagement and Initiatives at the National Computational Infrastructure. Dr. Evans oversees NCI's programs in highly-scalable computing, Data-intensive computing, data management and services, virtual laboratory innovation, and visualization. He has played leading roles in national virtual laboratories such as the Climate and Weather Science Laboratory (CWSLab) and VGL, as well as major international collaborations, such as the Unified Model infrastructure underpinning the ACCESS system for Climate and Weather, Earth Systems Grid Federation (ESGF), EarthCube, the Coupled Model Inter-comparison Project (CMIP), and its support for the Intergovernmental Panel on Climate Change (IPCC).</p>

<div class="divider-full">&nbsp;</div>

<p class="blue"><b>Lesley Wyborn</b> joined the then BMR in 1972 and for the next 42 years held a variety of science and geoinformatics positions as BMR changed to AGSO then Geoscience Australia. In 2014, Dr. Wyborn joined the ANU and currently has a joint adjunct fellowship with NCI and the Research School of Earth Sciences. She has been involved in many Australian eResearch projects, including the NeCTAR funded Virtual Geophysics Laboratory, the Virtual Hazards, Impacts and Risk Laboratory, and the Provenance Connectivity Projects. She is Deputy Chair of the Australian Academy of Science 'Data for Science Committee'. In 2014 she was awarded the Australian Public Service Medal for her contributions to Geoscience and Geoinformatics, and in 2015, the Geological Society of America, Geoinformatics Division 2015 Outstanding Career Achievement Award.</p>

<div class="divider-full">&nbsp;</div>
 <!-- Standard Copyright line here  -->

<div class="center">
<p class="footer">Copyright &reg; 2017 Jingbo Wang, Nicholas Car, Edward King, Ben Evans, and Lesley Wyborn</p>  
</div>

<div style="height:1px;background:#2b538e"></div>

</div>
</form>
</body>
</html> contentType 9 text/html url 54 http://www.dlib.org:80/dlib/january17/wang/01wang.html responseCode 3 200 